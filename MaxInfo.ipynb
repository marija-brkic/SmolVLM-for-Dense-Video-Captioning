{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5843d1bc-08a8-480b-b896-fd5dce846dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import clip\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def extract_all_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(frame_rgb)\n",
    "        frames.append(pil_image)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def get_clip_embeddings(frames):\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for frame in tqdm(frames, desc=\"Extracting CLIP features\"):\n",
    "            image_input = preprocess(frame).unsqueeze(0).to(device)\n",
    "            embedding = model.encode_image(image_input)\n",
    "            all_embeddings.append(embedding.cpu().numpy())\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "def truncated_svd(Q, s):\n",
    "    svd = TruncatedSVD(n_components=s)\n",
    "    Qs = svd.fit_transform(Q)\n",
    "    return Qs\n",
    "\n",
    "def rectangular_maxvol(Qs, r, tol=1e-6):\n",
    "    n, s = Qs.shape #total number of frames and features per frame\n",
    "    selected_indices = [] #selected indices\n",
    "    remaining = list(range(n)) #not yet selected indices\n",
    "    for _ in range(r): #num of indices I want to select\n",
    "        max_score = -np.inf\n",
    "        best_idx = -1\n",
    "        for i in remaining:\n",
    "            candidate = selected_indices + [i] #adds a new row to the already selected rows and checks \n",
    "            sub = Qs[candidate, :]\n",
    "            score = np.linalg.det(sub @ sub.T + tol * np.eye(len(candidate))) #checks if the determinant is larger and adds tolerance\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                best_idx = i\n",
    "        selected_indices.append(best_idx)\n",
    "        remaining.remove(best_idx)\n",
    "    return selected_indices\n",
    "\n",
    "def maxinfo_frame_selection(video_path, svd_dim=32, num_frames=32):\n",
    "    frames = extract_all_frames(video_path)\n",
    "    Q = get_clip_embeddings(frames)\n",
    "    Qs = truncated_svd(Q, s=svd_dim)\n",
    "    indices = rectangular_maxvol(Qs, r=num_frames)\n",
    "    selected_frames = [frames[i] for i in indices]\n",
    "    return selected_frames, indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b47655f3-4e5a-4fd1-b8fe-dec90201f3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting CLIP features: 100%|██████████| 375/375 [00:19<00:00, 19.43it/s]\n"
     ]
    }
   ],
   "source": [
    "video_path = \"Videos\\\\fragrance_on_the_trail_of_coco_mademoiselle_mp4.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c8ff9d0-913d-421c-99f1-0bdb6da6cba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting CLIP features: 100%|██████████| 375/375 [00:19<00:00, 19.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for i, frame in enumerate(selected_frames):\\n    print(f\"Frame index: {indices[i]}\")\\n    display(frame)'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "selected_frames, indices = maxinfo_frame_selection(video_path, svd_dim=32, num_frames=16)\n",
    "\n",
    "'''for i, frame in enumerate(selected_frames):\n",
    "    print(f\"Frame index: {indices[i]}\")\n",
    "    display(frame)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e11ed85-74b7-4150-b9fa-6845ee3d54e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4,  41,  48,  58,  74, 101, 103, 116, 178, 182, 185, 207, 231,\n",
       "       273, 290, 323])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.sort(indices)\n",
    "indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
